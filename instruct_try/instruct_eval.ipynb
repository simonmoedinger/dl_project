{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae7c534-0fb2-432b-a983-0e02f0c9550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/smoeding2/caches/'\n",
    "os.environ['XDG_CACHE_HOME'] = '/home/smoeding2/caches/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842dd5ff-83c7-436f-a6d2-0f5c44f5c914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smoeding2/.conda/envs/dl/lib/python3.11/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436a4dfae4bac7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T08:56:39.704999500Z",
     "start_time": "2023-11-24T08:56:23.049139300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"saved_models/chunk_instruct_tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a5be9de6fa057f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T08:56:39.870338700Z",
     "start_time": "2023-11-24T08:56:39.700985500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "tokenizer.pad_token_id=tokenizer.eos_token_id\n",
    "tokenizer.padding_size=\"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc56661-c3ed-48d2-b70d-8d04858f6c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09a4a774d574d3d9eb09b6102ddb846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d8ca2327f34cef91aa911fe05fad8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smoeding2/.conda/envs/dl/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32008, 4096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6be9cb7c17832f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T08:56:39.875651200Z",
     "start_time": "2023-11-24T08:56:39.870338700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generation_config=GenerationConfig(max_new_tokens=100,\n",
    "                                    temperature=0.2,\n",
    "                                    top_p=0.95,\n",
    "                                    top_k=40,\n",
    "                                    repetition_penalty=1.2,\n",
    "                                    bos_token_id=tokenizer.bos_token_id,\n",
    "                                    eos_token_id=tokenizer.eos_token_id,\n",
    "                                    do_sample=True,\n",
    "                                    use_cache=True,\n",
    "                                    output_attentions=False,\n",
    "                                    output_hidden_states=False,\n",
    "                                    output_scores=False,\n",
    "                                    remove_invalid_values=True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e996c8ae797740a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T08:56:39.879855700Z",
     "start_time": "2023-11-24T08:56:39.873644400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prompt_model(prompt):\n",
    "    input_tokens=tokenizer(f\"Du bist ein hilfreicher Assistent, der alle Fragen so korrekt wie möglich beantwortet. <BENUTZER>: {prompt} <ASSISTENT>: BEGINANTWORT\", return_tensors=\"pt\").to(model.device)\n",
    "    output_tokens=model.generate(**input_tokens, generation_config=generation_config, pad_token_id=tokenizer.eos_token_id)[0]\n",
    "    answer=tokenizer.decode(output_tokens)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ff80a7a740d601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T08:56:39.885267200Z",
     "start_time": "2023-11-24T08:56:39.878852100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "prompt_case = \"Es folgt eine Fallbeschreibung, dessen Anfang und Ende durch die Tags BEGINFALL und ENDFALL markiert ist. Darauf wird eine Frage gestellt, die zwischen den TAGS BEGINFRAGE und ENDFRAGE steht. Die Antwortmöglichkeiten stehen jeweils zwischen den Tags BEGINANTWORT und ENDANTWORT. BEGINFALL <INSERTFALL> ENDFALL BEGINFRAGE <INSERTFRAGE> ENDFRAGE BEGINANTWORT <INSERTANTWORT0> ENDANTWORT BEGINANTWORT <INSERTANTWORT1> ENDANTWORT BEGINANTWORT <INSERTANTWORT2> ENDANTWORT BEGINANTWORT <INSERTANTWORT3> ENDANTWORT BEGINANTWORT <INSERTANTWORT4> ENDANTWORT. Beantworte die Frage, indem du die korrekte Antwortmöglichkeit wiedergibst.\"\n",
    "prompt_no_case = \"Es wird eine Frage gestellt, die zwischen den TAGS BEGINFRAGE und ENDFRAGE steht. Die Antwortmöglichkeiten stehen jeweils zwischen den Tags BEGINANTWORT und ENDANTWORT. BEGINFALL <INSERTFALL> ENDFALL BEGINFRAGE <INSERTFRAGE> ENDFRAGE BEGINANTWORT <INSERTANTWORT0> ENDANTWORT BEGINANTWORT <INSERTANTWORT1> ENDANTWORT BEGINANTWORT <INSERTANTWORT2> ENDANTWORT BEGINANTWORT <INSERTANTWORT3> ENDANTWORT BEGINANTWORT <INSERTANTWORT4> ENDANTWORT. Beantworte die Frage, indem du die korrekte Antwortmöglichkeit wiedergibst.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aee5cfd96cc34947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T08:56:40.307731400Z",
     "start_time": "2023-11-24T08:56:40.292352700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(\"output.csv\").head(5)\n",
    "questions_df[\"Prediction LLM\"] = [-1 for i in range(len(questions_df))]\n",
    "questions_df[\"Answer LLM\"] = [-1 for i in range(len(questions_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d549f03f-c8d8-41e3-b05a-d34de9df3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aba98c46cd6efe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T09:12:17.627536Z",
     "start_time": "2023-11-24T09:10:23.737879900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Du bist ein hilfreicher Assistent, der alle Fragen so korrekt wie möglich beantwortet.<BENUTZER>: Es folgt eine Fallbeschreibung, dessen Anfang und Ende durch die TagsBEGINFALL undENDFALL markiert ist. Darauf wird eine Frage gestellt, die zwischen den TAGSBEGINFRAGE undENDFRAGE steht. Die Antwortmöglichkeiten stehen jeweils zwischen den TagsBEGINANTWORT undENDANTWORT.BEGINFALL Eine 35-jährige Patientin wird in Begleitung des Notarztes in die Klinik gebracht. Die Patientin war auf ihrem Pferd geritten. Dies ging durch, und die Patientin ist mit der rechten Körperhälfte mit hoher Geschwindigkeit gegen ein Verkehrsschild geprallt. Bei Eintreffen des Notarztes GCS 8. Intubation vor Ort und Verbringen in das nächstgelegene Krankenhaus. Übergabe durch den Notarzt: intubierte Patientin, hochgradig volumenbedürftig mit hochlaufenden kreislaufwirksamen Medikamenten (Adrenalin). In der Untersuchung: Patientin im Stiffneck auf der Vakuummatratze, Pupillen anisokor, links größer als rechts. Lichtreagibel. Auskultation des Thorax: links abgeschwächtes Atemgeräusch. Becken stabil, Extremitäten nicht verletzt. Keine äußeren Blutungen. Abdomen prall. Aktuell Blutdruck 80/30 mmHg. Herzfrequenz 140/min. Erste arterielle Blutgasanalyse: pH-Wert 7,37, pCO<sub>2</sub> 35 mmHg, pO<sub>2</sub> 125 mmHg, Bikarbonat 25 mmol/L, BE -1 mmol/L. O<sub>2</sub>-Sättigung 99 %. Hb 42 g/L.ENDFALLBEGINFRAGE Welche der genannten Vorgehensweisen ist bei der Patientin als Nächstes am ehesten indiziert?ENDFRAGEBEGINANTWORT  Anlage einer HirndrucksondeENDANTWORTBEGINANTWORT  Polytrauma-CT-SpiraleENDANTWORTBEGINANTWORT  MRT des AbdomensENDANTWORTBEGINANTWORT  Anlage einer Thoraxdrainage linksENDANTWORTBEGINANTWORT  12-Kanal-EKGENDANTWORT. Beantworte die Frage, indem du die korrekte Antwortmöglichkeit wiedergibst.<ASSISTENT>:BEGINANTWORT</s><s> Wie kann man sich selbstständig machen ohne Kapital? Diese Frage stellen sich viele Gründerinnen und Gründer, wenn sie ihre Idee verwirklichen wollen. Denn oftmals fehlt es an finanzieller Unterstützung oder auch schlichtweg an demENDANTWORTigen Startkapital für die Selbständigkeit.\n",
      "Die gute Nachricht zuerst: Auch ohne große Summen können Sie Ihre Ex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:17, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<INSERTANTWORT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m,row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt_model(prompt))\n\u001b[0;32m---> 11\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<ASSISTENT>: BEGINANTWORT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENDANTWORT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(questions_df.iterrows()):\n",
    "    if row[\"Answer LLM\"] == -1:\n",
    "        if type(row[\"Case\"]) == str:\n",
    "            prompt = prompt_case.replace(\"<INSERTFALL>\",row[\"Case\"])\n",
    "        else:\n",
    "            prompt = prompt_no_case\n",
    "        prompt = prompt.replace(\"<INSERTFRAGE>\",row[\"Question\"])\n",
    "        for i in range(5):\n",
    "            prompt = prompt.replace(\"<INSERTANTWORT\" + str(i) + \">\",row[\"Answer \"+ str(i+1)].replace(str(i) + \")\",\"\"))\n",
    "        print(prompt_model(prompt))\n",
    "        answer = prompt_model(prompt).split(\"<ASSISTENT>: BEGINANTWORT\")[1].split(\"ENDANTWORT\")\n",
    "        found = False\n",
    "        for i in range(5):\n",
    "            if row[\"Answer \"+ str(i+1)].replace(str(i) + \")\",\"\").strip().lower() == answer[0].strip().lower():\n",
    "                found = i\n",
    "                break\n",
    "        #print(answer)\n",
    "        if found != False:\n",
    "            questions_df.loc[index, \"Answer LLM\"] = found\n",
    "        questions_df.loc[index, \"Prediction LLM\"] = \" ENDANTWORT \".join(answer)\n",
    "print(\"Values set:\", len(questions_df[questions_df[\"Answer LLM\"] != -1]),\"/\",len(questions_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5194e3f9c58349c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T09:13:35.673403Z",
     "start_time": "2023-11-24T09:13:35.666689300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "questions_df.to_csv(\"leo_mistral_hessiani_output.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43cb16ba034b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T09:12:46.483444Z",
     "start_time": "2023-11-24T09:12:46.478481300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = questions_df[questions_df[\"Answer LLM\"] != -1][\"Answer LLM\"]\n",
    "y_true = questions_df[questions_df[\"Answer LLM\"] != -1][\"Correct Answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6ec3d09c98719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T09:13:10.402273900Z",
     "start_time": "2023-11-24T09:13:10.398224300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
